{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cemlus/Cognitive-Computing/blob/main/Assignment_9_102316047.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "747jdH8f8_Sn"
      },
      "source": [
        "**Siddhant (102316117) -> Assignment 9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2G2FZ4v8_Sp"
      },
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuation.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribution (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXjC5sdS9lp2",
        "outputId": "0013bbce-cbab-44b9-c1ae-b98dda1d3e65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNUQbbh18_Sq",
        "outputId": "19c466f2-d71a-448d-a2d6-7aed604bc88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences:\n",
            "['Technology is evolving faster than ever before, reshaping the way we live, work, and connect.', 'Innovations like artificial intelligence, virtual reality, and blockchain are transforming industries.', 'Smartphones have become our personal assistants, connecting us instantly to information and people.', 'Yet, this rapid growth also brings challenges like data privacy and ethical concerns.', 'Embracing technology wisely will define the future of humanity.']\n",
            "\n",
            "Filtered Words:\n",
            "['technology', 'evolving', 'faster', 'ever', 'reshaping', 'way', 'live', 'work', 'connect', 'innovations', 'like', 'artificial', 'intelligence', 'virtual', 'reality', 'blockchain', 'transforming', 'industries', 'smartphones', 'become', 'personal', 'assistants', 'connecting', 'us', 'instantly', 'information', 'people', 'yet', 'rapid', 'growth', 'also', 'brings', 'challenges', 'like', 'data', 'privacy', 'ethical', 'concerns', 'embracing', 'technology', 'wisely', 'define', 'future', 'humanity']\n",
            "\n",
            "Word Frequency Distribution:\n",
            "\n",
            "Word Frequency Table:\n",
            "technology: 2\n",
            "evolving: 1\n",
            "faster: 1\n",
            "ever: 1\n",
            "reshaping: 1\n",
            "way: 1\n",
            "live: 1\n",
            "work: 1\n",
            "connect: 1\n",
            "innovations: 1\n",
            "like: 2\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "virtual: 1\n",
            "reality: 1\n",
            "blockchain: 1\n",
            "transforming: 1\n",
            "industries: 1\n",
            "smartphones: 1\n",
            "become: 1\n",
            "personal: 1\n",
            "assistants: 1\n",
            "connecting: 1\n",
            "us: 1\n",
            "instantly: 1\n",
            "information: 1\n",
            "people: 1\n",
            "yet: 1\n",
            "rapid: 1\n",
            "growth: 1\n",
            "also: 1\n",
            "brings: 1\n",
            "challenges: 1\n",
            "data: 1\n",
            "privacy: 1\n",
            "ethical: 1\n",
            "concerns: 1\n",
            "embracing: 1\n",
            "wisely: 1\n",
            "define: 1\n",
            "future: 1\n",
            "humanity: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"\"\"Technology is evolving faster than ever before, reshaping the way we live, work, and connect.\n",
        "Innovations like artificial intelligence, virtual reality, and blockchain are transforming industries.\n",
        "Smartphones have become our personal assistants, connecting us instantly to information and people.\n",
        "Yet, this rapid growth also brings challenges like data privacy and ethical concerns.\n",
        "Embracing technology wisely will define the future of humanity.\"\"\"\n",
        "\n",
        "text_lower = text.lower()\n",
        "text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 2. Tokenize\n",
        "words = word_tokenize(text_no_punct)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# 4. Word frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "# Display outputs\n",
        "print(\"Tokenized Sentences:\")\n",
        "print(sentences)\n",
        "print(\"\\nFiltered Words:\")\n",
        "print(filtered_words)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "# Print word frequencies\n",
        "print(\"\\nWord Frequency Table:\")\n",
        "for word, freq in fdist.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8TklRNp8_St"
      },
      "source": [
        "Q2: Stemming and Lemmatization\n",
        "1. Take the tokenized words from Question 1 (after stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply Lemmatization using NLTK's WordNetLemmatizer.\n",
        "4. Compare and display results of both techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF78V-JB8_Su",
        "outputId": "cf8e3a9e-7661-415c-ed7b-639d5c85acda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered Words:\n",
            "['technology', 'evolving', 'faster', 'ever', 'reshaping', 'way', 'live', 'work', 'connect', 'innovations', 'like', 'artificial', 'intelligence', 'virtual', 'reality', 'blockchain', 'transforming', 'industries', 'smartphones', 'become', 'personal', 'assistants', 'connecting', 'us', 'instantly', 'information', 'people', 'yet', 'rapid', 'growth', 'also', 'brings', 'challenges', 'data', 'privacy', 'ethical', 'concerns', 'embracing', 'technology', 'wisely', 'define', 'future', 'humanity']\n",
            "Original Word        Porter Stemmer       Lancaster Stemmer    Lemmatizer          \n",
            "--------------------------------------------------------------------------------\n",
            "technology           technolog            technolog            technology          \n",
            "evolving             evolv                evolv                evolving            \n",
            "faster               faster               fast                 faster              \n",
            "ever                 ever                 ev                   ever                \n",
            "reshaping            reshap               reshap               reshaping           \n",
            "way                  way                  way                  way                 \n",
            "live                 live                 liv                  live                \n",
            "work                 work                 work                 work                \n",
            "connect              connect              connect              connect             \n",
            "innovations          innov                innov                innovation          \n",
            "like                 like                 lik                  like                \n",
            "artificial           artifici             art                  artificial          \n",
            "intelligence         intellig             intellig             intelligence        \n",
            "virtual              virtual              virt                 virtual             \n",
            "reality              realiti              real                 reality             \n",
            "blockchain           blockchain           blockchain           blockchain          \n",
            "transforming         transform            transform            transforming        \n",
            "industries           industri             industry             industry            \n",
            "smartphones          smartphon            smartphon            smartphones         \n",
            "become               becom                becom                become              \n",
            "personal             person               person               personal            \n",
            "assistants           assist               assist               assistant           \n",
            "connecting           connect              connect              connecting          \n",
            "us                   us                   us                   u                   \n",
            "instantly            instantli            inst                 instantly           \n",
            "information          inform               inform               information         \n",
            "people               peopl                peopl                people              \n",
            "yet                  yet                  yet                  yet                 \n",
            "rapid                rapid                rapid                rapid               \n",
            "growth               growth               grow                 growth              \n",
            "also                 also                 also                 also                \n",
            "brings               bring                bring                brings              \n",
            "challenges           challeng             challeng             challenge           \n",
            "data                 data                 dat                  data                \n",
            "privacy              privaci              priv                 privacy             \n",
            "ethical              ethic                eth                  ethical             \n",
            "concerns             concern              concern              concern             \n",
            "embracing            embrac               embrac               embracing           \n",
            "technology           technolog            technolog            technology          \n",
            "wisely               wise                 wis                  wisely              \n",
            "define               defin                defin                define              \n",
            "future               futur                fut                  future              \n",
            "humanity             human                hum                  humanity            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# filtered_words from Question 1\n",
        "print(\"\\nFiltered Words:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# Init stemmers and lemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply Stemming and Lemmatization\n",
        "porter_stemmed = [porter.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Display Results\n",
        "print(\"{:<20} {:<20} {:<20} {:<20}\".format('Original Word', 'Porter Stemmer', 'Lancaster Stemmer', 'Lemmatizer'))\n",
        "print(\"-\"*80)\n",
        "for original, porter_w, lancaster_w, lemma_w in zip(filtered_words, porter_stemmed, lancaster_stemmed, lemmatized):\n",
        "    print(\"{:<20} {:<20} {:<20} {:<20}\".format(original, porter_w, lancaster_w, lemma_w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgRNzV548_Sv"
      },
      "source": [
        "Q3. Regular Expressions and Text Splitting\n",
        "1. Take their original text from Question 1.\n",
        "2. Use regular expressions to:\n",
        "\n",
        "  a. Extract all words with more than 5 letters.\n",
        "\n",
        "  b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliÆ«ng techniques to:\n",
        "\n",
        "  a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "\n",
        "  b. Extract words starting with a vowel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRc8qgU08_Sv",
        "outputId": "96a70deb-204b-433e-d9a7-2a3a2e6fba8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with more than 5 letters:\n",
            " ['Technology', 'evolving', 'faster', 'before', 'reshaping', 'connect', 'Innovations', 'artificial', 'intelligence', 'virtual', 'reality', 'blockchain', 'transforming', 'industries', 'Smartphones', 'become', 'personal', 'assistants', 'connecting', 'instantly', 'information', 'people', 'growth', 'brings', 'challenges', 'privacy', 'ethical', 'concerns', 'Embracing', 'technology', 'wisely', 'define', 'future', 'humanity']\n",
            "\n",
            "Numbers found in the text:\n",
            " []\n",
            "\n",
            "Capitalized words:\n",
            " ['Technology', 'Innovations', 'Smartphones', 'Yet', 'Embracing']\n",
            "\n",
            "Words containing only alphabets:\n",
            " ['Technology', 'is', 'evolving', 'faster', 'than', 'ever', 'before', 'reshaping', 'the', 'way', 'we', 'live', 'work', 'and', 'connect', 'Innovations', 'like', 'artificial', 'intelligence', 'virtual', 'reality', 'and', 'blockchain', 'are', 'transforming', 'industries', 'Smartphones', 'have', 'become', 'our', 'personal', 'assistants', 'connecting', 'us', 'instantly', 'to', 'information', 'and', 'people', 'Yet', 'this', 'rapid', 'growth', 'also', 'brings', 'challenges', 'like', 'data', 'privacy', 'and', 'ethical', 'concerns', 'Embracing', 'technology', 'wisely', 'will', 'define', 'the', 'future', 'of', 'humanity']\n",
            "\n",
            "Words starting with a vowel:\n",
            " ['is', 'evolving', 'ever', 'and', 'Innovations', 'artificial', 'intelligence', 'and', 'are', 'industries', 'our', 'assistants', 'us', 'instantly', 'information', 'and', 'also', 'and', 'ethical', 'Embracing', 'of']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# 1a. Extract all words with more than 5 letters\n",
        "words_more_than_5 = re.findall(r'\\b[a-zA-Z]{6,}\\b', text)\n",
        "print(\"Words with more than 5 letters:\\n\", words_more_than_5)\n",
        "\n",
        "# 1b. Extract all numbers\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "print(\"\\nNumbers found in the text:\\n\", numbers)\n",
        "\n",
        "# 1c. Extract all capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "print(\"\\nCapitalized words:\\n\", capitalized_words)\n",
        "\n",
        "# 2a. Split text into words with only alphabets (remove digits and special characters)\n",
        "only_alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "print(\"\\nWords containing only alphabets:\\n\", only_alpha_words)\n",
        "\n",
        "# 2b. Extract words starting with a vowel (from only alphabetic words)\n",
        "vowel_words = [word for word in only_alpha_words if word.lower().startswith(('a', 'e', 'i', 'o', 'u'))]\n",
        "print(\"\\nWords starting with a vowel:\\n\", vowel_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lJfF2NW8_Sw"
      },
      "source": [
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "1. Take original text from Question 1.\n",
        "2. Write a custom tokenization function that:\n",
        "a. Removes punctuation and special symbols, but keeps contractions (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "3. Use Regex Substitutions (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4cj3w6L8_Sw",
        "outputId": "5863d868-630e-4a8e-b1e2-de778ec1e665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokenized Text: ['technology', 'is', 'evolving', 'faster', 'than', 'ever', 'before', 'reshaping', 'the', 'way', 'we', 'live', 'work', 'and', 'connect', 'innovations', 'like', 'artificial', 'intelligence', 'virtual', 'reality', 'and', 'blockchain', 'are', 'transforming', 'industries', 'smartphones', 'have', 'become', 'our', 'personal', 'assistants', 'connecting', 'us', 'instantly', 'to', 'information', 'and', 'people', 'yet', 'this', 'rapid', 'growth', 'also', 'brings', 'challenges', 'like', 'data', 'privacy', 'and', 'ethical', 'concerns', 'embracing', 'technology', 'wisely', 'will', 'define', 'the', 'future', 'of', 'humanity']\n",
            "\n",
            "Text with Emails, URLs, and Phone Numbers Replaced:\n",
            "\n",
            "Technology is evolving faster than ever before, reshaping the way we live, work, and connect. \n",
            "Innovations like artificial intelligence, virtual reality, and blockchain are transforming industries. \n",
            "Smartphones have become our personal assistants, connecting us instantly to information and people. \n",
            "Yet, this rapid growth also brings challenges like data privacy and ethical concerns. \n",
            "Embracing technology wisely will define the future of humanity.\n"
          ]
        }
      ],
      "source": [
        "# custom tokenization function\n",
        "def custom_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
        "    return word_tokenize(text)\n",
        "\n",
        "custom_tokens = custom_tokenizer(text)\n",
        "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
        "\n",
        "# a\n",
        "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
        "# b\n",
        "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
        "# c\n",
        "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
        "\n",
        "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\\n\")\n",
        "print(text_cleaned)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}